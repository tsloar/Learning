---
title: "Grouped summaries with summaries ()"
author: "Tetiana Matviichuk"
date: "26 06 2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Loading libraries
```{r, message = FALSE}
library(tidyverse)
library(nycflights13)
```

# How to explore the relationship between the distance and average delay for each location?



There are three steps to prepare this data:

1. Group flights by destination.

2. Summarise to compute distance, average delay, and number of flights.

3. Filter to remove noisy points and Honolulu airport, which is almost twice as far away as the next closest airport.

### summarize() for data frame
It collapses a data frame to a single row:

```{r}
summarise(flights, delay = mean(dep_delay, na.rm = TRUE))
```

But it is more useful with group_by():
```{r}
by_day <- group_by(flights, year, month, day)
summarise(by_day, delay = mean(dep_delay, na.rm = TRUE))
```

### Combining multiple operations with the pipe

```{r}
by_dest <- flights %>%
        group_by(dest) %>%
        summarise(
                  count = n(),
                  dist = mean(distance, na.rm = TRUE),
                  delay = mean(arr_delay, na.rm = TRUE)
                  ) %>%
        filter(count > 20, dest != "HNL")  #to ignore data with small observations

```

### Presenting plot. 

It looks like delays increase with distance up to ~750 miles and then decrease. Maybe as flights get longer there's more ability to make up delays in the air?

```{r}
ggplot(data = by_dest, mapping = aes(x = dist, y = delay)) +
        geom_point(aes(size = count, color = count), alpha = 1/3) +
        geom_smooth(se = FALSE)
```


## Missing Values

We could also tackle the problem by first removing the cancelled flights.

```{r}

not_cancelled <- flights %>% 
  filter(!is.na(dep_delay), !is.na(arr_delay))

not_cancelled %>% 
  group_by(year, month, day) %>% 
  summarise(mean = mean(dep_delay))

```

How many are cancelled flights?

```{r}
cancelled <- flights %>%
        filter(is.na(dep_delay), is.na(arr_delay))
cancelled[which(cancelled$dep_delay == FALSE), ]
cancelled %>%
        group_by(tailnum) %>%
        summarise(
                count = n()
                )
```


### Counts

Whenever you do any aggregation, it’s always a good idea to include either a count (n()), or a count of non-missing values (sum(!is.na(x))). That way you can check that you’re not drawing conclusions based on very small amounts of data. 

```{r}
delays <- flights %>%
        group_by(tailnum) %>%
        summarise(
                delay = mean(arr_delay)
        )
head(delays, 20)

ggplot(data = delays, mapping = aes(x = delay)) +
        geom_freqpoly(binwidth = 10)
```

But we can get more insight if we draw a scatterplot of number of flights vs. average delay:
```{r}
delays <- not_cancelled%>%
  group_by(tailnum) %>%
  summarise(
    count = n(),
    average_delay = mean(arr_delay, na.rm= TRUE)
  )

ggplot(data = delays, mapping = aes(x = count, y = average_delay)) +
  geom_point(aes(color = count), alpha = 1/10)
```


When looking at this sort of plot, it’s often useful to filter out the groups with the smallest numbers of observations, so you can see more of the pattern and less of the extreme variation in the smallest groups.

```{r}
delays %>%
  filter(count > 25) %>%
  ggplot(mapping = aes(x = count, y = average_delay)) +
  geom_point(aes(color = count), alpha = 1/10)


```


**RStudio tip**: a useful keyboard shortcut is **Cmd/Ctrl + Shift + P**. This resends the previously sent chunk from the editor to the console. 


## Example on Lahman database 

There’s another common variation of this type of pattern. Let’s look at how the average performance of batters in baseball is related to the number of times they’re at bat. 

We will compute the batting average (number of hits / number of attempts) of every major league baseball player.

When plotting the skill of the batter (measured by the batting average, *ba*) against the number of opportunities to hit the ball (measured by at bat, *ab*), see two patterns:

1. As above, the variation in our aggregate decreases as we get more data points.

2. There’s a positive correlation between skill (ba) and opportunities to hit the ball (ab). This is because teams control who gets to play, and obviously they’ll pick their best players.

```{r}
library(Lahman)
library(dplyr)

batting <- Lahman::Batting
batters <- batting %>%
  group_by(playerID) %>%
  summarise(
    ba = sum(H, na.rm = T)/sum(AB, na.rm = T),
    ab = sum(AB, na.rm = T)
  )

batters %>%
  filter(ab > 100) %>%
  ggplot(mapping = aes(x = ab, y = ba)) +
  geom_point(aes(color = ab), alpha = 1/10) +
  geom_smooth(se = FALSE)

```

## Useful summary function

Except of sum(x), mean(x), counts, there are:

+ **Measures of location**: *the median* - is a value where 50% of x is above it, and 50% is below it.

+ **Measures of spread**: sd(x), IQR(x), mad(x). 
        *The root mean squared deviation*, or standard deviation *sd(x)*, is the standard measure of spread. 
        *The interquartile range IQR(x)* and *median absolute deviation mad(x)* are robust equivalents that may be more useful if you have outliers
  
+ **Measures of rank**: *min(x), quantile(x, 0.25), max(x)*. 
         Quantiles are a generalisation of the median. For example, quantile(x, 0.25) will find a value of x that is greater than 25% of the values, 
        and less than the remaining 75%.

+ **Measures of position**: first(x), nth(x, 2), last(x). These work similarly to x[1], x[2], and x[length(x)]
```{r}
not_cancelled %>% 
  group_by(year, month, day) %>% 
  summarise(
    first_dep = first(dep_time), 
    last_dep = last(dep_time)
  )
```

        These functions are complementary to filtering on ranks. Filtering gives you all variables, with each observation in a separate row:

```{r}
not_cancelled %>% 
  group_by(year, month, day) %>% 
  mutate(r = min_rank(desc(dep_time))) %>% 
  filter(r %in% range(r))
```

+ **Counts**: n() - which takes no arguments, and returns the size of the current group. 
        To count the number of non-missing values, use sum(!is.na(x)). 
        To count the number of distinct (unique) values, use n_distinct(x).
            
```{r}
# Which destinations have the most carriers?
not_cancelled %>% 
  group_by(dest) %>% 
  summarise(carriers = n_distinct(carrier)) %>% 
  arrange(desc(carriers))
```

        Counts are so useful that dplyr provides a simple helper if all you want is a count:

```{r}
not_cancelled %>% 
  count(dest)
```
           
        You can optionally provide a weight variable. For example, you could use this to “count” (sum) the total number of miles a plane flew:
```{r}
not_cancelled %>% 
  count(tailnum, wt = distance)
```

+ **Counts and proportions of logical values**: sum(x > 10), mean(y == 0). 
        When used with numeric functions, TRUE is converted to 1 and FALSE to 0. This makes sum() and mean() very useful: sum(x) gives the number of TRUEs in x, and 
        mean(x) gives the proportion.
        
```{r}
# How many flights left before 5am? (these usually indicate delayed
# flights from the previous day)
not_cancelled %>% 
  group_by(year, month, day) %>% 
  summarise(n_early = sum(dep_time < 500))

# What proportion of flights are delayed by more than an hour?
not_cancelled %>% 
  group_by(year, month, day) %>% 
  summarise(hour_prop = mean(arr_delay > 60))
```

## Grouping by multiple variables

When you group by multiple variables, each summary peels off one level of the grouping. That makes it easy to progressively roll up a dataset:

```{r}
daily <- group_by(flights, year, month, day)
(per_day   <- summarise(daily, flights = n()))
(per_month <- summarise(per_day, flights = sum(flights)))
(per_year <- summarise(per_month, flights = sum(flights)))
```

 It’s OK for sums and counts - the sum of groupwise sums is the overall sum, but the median of groupwise medians is not the overall median.
 
## Ungrouping

```{r}

daily %>% 
  ungroup() %>%             # no longer grouped by date
  summarise(flights = n())  # all flights

```
